{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how we can perfom text classification with spaCy\n",
    "The dataset is from the Tweet Sentiment Extraction challenge from Kaggle(https://www.kaggle.com/c/tweet-sentiment-extraction/overview)\n",
    "We would perform text classification using spaCy on tweet data to classify tweets as \"positive\",\"negative\"  or \"neutral\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required libraries\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "import sys\n",
    "from spacy import displacy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from spacy.tokens import DocBin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define methods to pre-process the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    " # converting return value from list to string\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    \n",
    "    return text2.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from : https://medium.com/analytics-vidhya/building-a-text-classifier-with-spacy-3-0-dd16e9979a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(file_path):\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels \n",
    "    and transform them in spacy documents\n",
    "    \n",
    "    data: list(tuple(text, label))\n",
    "    \n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    train_data = pd.read_csv(file_path)\n",
    "    train_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "    train_data['Num_words_text'] = train_data['text'].apply(lambda x:len(str(x).split())) \n",
    "    mask = train_data['Num_words_text'] >2\n",
    "    train_data = train_data[mask]\n",
    "    print(train_data['sentiment'].value_counts())\n",
    "    \n",
    "    train_data['text'] = train_data['text'].apply(remove_emoji)\n",
    "    train_data['text'] = train_data['text'].apply(remove_url)\n",
    "    train_data['text'] = train_data['text'].apply(clean_text)\n",
    "   \n",
    "    data = tuple(zip(train_data['text'].tolist(), train_data['sentiment'].tolist())) \n",
    "    print(data[1])\n",
    "    docs = []\n",
    "    # nlp.pipe([texts]) is way faster than running \n",
    "    # nlp(text) for each text\n",
    "    # as_tuples allows us to pass in a tuple, \n",
    "    # the first one is treated as text\n",
    "    # the second one will get returned as it is.\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        #print(label)\n",
    "        if (label=='positive'):\n",
    "            doc.cats['positive'] = 1\n",
    "            doc.cats['negative'] = 0\n",
    "            doc.cats['neutral']  = 0\n",
    "        elif (label=='negative'):\n",
    "            doc.cats['positive'] = 0\n",
    "            doc.cats['negative'] = 1\n",
    "            doc.cats['neutral']  = 0\n",
    "        else:\n",
    "            doc.cats['positive'] = 0\n",
    "            doc.cats['negative'] = 0\n",
    "            doc.cats['neutral']  = 1\n",
    "        #print(doc.cats)\n",
    "        \n",
    "        # put them into a nice list\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs,train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert our train data and test data to spaCy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     10704\n",
      "positive     8375\n",
      "negative     7673\n",
      "Name: sentiment, dtype: int64\n",
      "('sooo will miss here diego', 'negative')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf0ba01054f4e42a21d6d95217505b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     1376\n",
      "positive    1075\n",
      "negative     983\n",
      "Name: sentiment, dtype: int64\n",
      "('shanghai also really exciting precisely skyscrapers galore good tweeps china', 'positive')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b474776915d246ca9f2b016906a690f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_docs,train_data  = make_docs(\"C:\\\\TweetSenitment\\\\train.csv\")\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./textcat_data/textcat_train.spacy\")\n",
    "\n",
    "test_docs,test_data  = make_docs(\"C:\\\\TweetSenitment\\\\test.csv\")\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=test_docs)\n",
    "doc_bin.to_disk(\"./textcat_data/textcat_valid.spacy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the model on our dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create base config file from here:https://spacy.io/usage/training#quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "textcat_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train textcat_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ./textcat_base_config.cfg ./textcat_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['transformer', 'textcat']\n",
      "[i] Initial learn rate: 0.0\n",
      "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ------------  ----------  ------\n",
      "  0       0           0.00          0.17        0.00    0.00\n",
      "  1     200           0.10         43.59       69.74    0.70\n",
      "  2     400           0.35         37.37       71.49    0.71\n",
      "  3     600           0.50         28.74       70.67    0.71\n",
      "  4     800           0.36         22.46       73.57    0.74\n",
      "  5    1000           0.75         20.60       72.50    0.72\n",
      "  7    1200           0.33         11.35       71.95    0.72\n",
      "  8    1400           0.42         11.61       72.84    0.73\n",
      "  9    1600           0.70         11.11       71.96    0.72\n",
      "[+] Saved pipeline to output directory\n",
      "textcat_output\\model-last"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-11 14:25:14,792] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "[2021-06-11 14:25:16,108] [INFO] Set up nlp object from config\n",
      "[2021-06-11 14:25:16,116] [DEBUG] Loading corpus from path: textcat_data\\textcat_valid.spacy\n",
      "[2021-06-11 14:25:16,117] [DEBUG] Loading corpus from path: textcat_data\\textcat_train.spacy\n",
      "[2021-06-11 14:25:16,117] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2021-06-11 14:25:16,120] [INFO] Created vocabulary\n",
      "[2021-06-11 14:25:16,120] [INFO] Finished initializing nlp object\n",
      "[2021-06-11 14:25:44,641] [INFO] Initialized pipeline components: ['transformer', 'textcat']\n",
      "[2021-06-11 14:25:44,666] [DEBUG] Loading corpus from path: textcat_data\\textcat_valid.spacy\n",
      "[2021-06-11 14:25:44,667] [DEBUG] Loading corpus from path: textcat_data\\textcat_train.spacy\n",
      "[2021-06-11 14:25:44,704] [DEBUG] Removed existing output directory: textcat_output\\model-best\n",
      "[2021-06-11 14:25:44,709] [DEBUG] Removed existing output directory: textcat_output\\model-last\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train textcat_config.cfg --verbose --output ./textcat_output --paths.train textcat_data/textcat_train.spacy --paths.dev textcat_data/textcat_valid.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our model on  test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: want david cook\n",
      "Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.06087260693311691, 'negative': 0.05941738560795784, 'neutral': 0.8797099590301514}\n",
      "=======================================\n",
      "Text: okaii cool cant wait series begin guna awesome\n",
      " Orig Cat:positive\n",
      " Predicted Cats:\n",
      "{'positive': 0.9591326713562012, 'negative': 0.00452009541913867, 'neutral': 0.036347199231386185}\n"
     ]
    }
   ],
   "source": [
    "nlp_textcat = spacy.load(\"textcat_output/model-best\")\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_cats = test_data['sentiment'].tolist()\n",
    "doc2 = nlp_textcat(test_texts[100])\n",
    "print(\"Text: \"+ test_texts[100])\n",
    "print(\"Orig Cat:\"+ test_cats[100])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)\n",
    "print(\"=======================================\")\n",
    "doc2 = nlp_textcat(test_texts[1000])\n",
    "print(\"Text: \"+ test_texts[1000])\n",
    "print(\" Orig Cat:\"+test_cats[1000])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc2.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.9590757489204407, 'negative': 0.004502081777900457, 'neutral': 0.036422159522771835}\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp_textcat(\"Avengers Endgame was a great movie\")\n",
    "print(doc2.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.02091258205473423, 'negative': 0.8962381482124329, 'neutral': 0.0828491747379303}\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp_textcat(\"Data science is tough to master\")\n",
    "print(doc2.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
